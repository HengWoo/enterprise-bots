# v0.5.0 Integration Plan

**Date:** 2025-10-31
**Status:** Implementation Ready
**Goal:** Integrate verification and code generation modules into the agent workflow

---

## Current Status

**Modules Built:** ✅
- `src/verification/` - 2,205 lines, 179 tests passing
- `src/codegen/` - 1,691 lines, ready for use

**Integration Status:** ⏳
- Modules exist in Docker container
- Not connected to agent workflow
- Not accessible from bot prompts

---

## Integration Architecture

### Two Integration Approaches

**Approach 1: Tool-Based Integration** (Recommended)
- Create MCP tools that wrap verification/codegen modules
- Bots call tools explicitly via agent SDK
- Full control, explicit invocation, easy to test

**Approach 2: Middleware Integration**
- Inject verification into message processing pipeline
- Automatic verification of all responses
- Transparent to bots, always active

**Decision: Use Approach 1 (Tool-Based)**
- More explicit and controllable
- Easier to test and debug
- Follows existing MCP architecture
- Can evolve to Approach 2 later if needed

---

## Implementation Steps

### Step 1: Create Verification MCP Tools

**File:** `src/tools/verification_tools.py` (NEW)

**Tools to create:**
1. `validate_calculation` - Verify mathematical calculations
2. `verify_financial_data` - Verify financial balance equations
3. `verify_data_quality` - Check for missing/invalid data
4. `verify_html_output` - Validate HTML structure

**Example:**
```python
@mcp_tool(name="validate_calculation", description="Verify mathematical calculations")
async def validate_calculation_tool(
    operation: str,
    inputs: dict,
    expected_result: float
) -> dict:
    """
    Verify that a calculation is mathematically sound.

    Args:
        operation: Type of calculation (e.g., "profit_margin", "percentage")
        inputs: Input values (e.g., {"revenue": 100, "cost": 120})
        expected_result: Expected result to verify

    Returns:
        {"valid": bool, "warnings": list, "errors": list}
    """
    from src.verification import verify_financial_balance, verify_positive_number

    warnings = []
    errors = []

    # Verify inputs are valid
    if operation == "profit_margin":
        if inputs.get("cost", 0) > inputs.get("revenue", 0):
            errors.append("Cost exceeds revenue - impossible profit margin")

    # Return verification result
    return {
        "valid": len(errors) == 0,
        "warnings": warnings,
        "errors": errors
    }
```

---

### Step 2: Create Code Generation MCP Tools

**File:** `src/tools/codegen_tools.py` (NEW)

**Tools to create:**
1. `generate_python_code` - Generate Python analysis code
2. `generate_sql_query` - Generate SQL queries (read-only)
3. `validate_code` - Run ruff + mypy linting
4. `execute_code_safe` - Execute in sandbox

**Example:**
```python
@mcp_tool(name="generate_python_code", description="Generate Python code for data analysis")
async def generate_python_code_tool(
    task_description: str,
    data_context: dict
) -> dict:
    """
    Generate Python code using templates.

    Args:
        task_description: What the code should do
        data_context: Context about data (columns, types, etc.)

    Returns:
        {"code": str, "lint_status": str, "warnings": list}
    """
    from src.codegen import CodeGenerator, CodeValidator

    # Generate code from template
    generator = CodeGenerator()
    code = generator.generate(
        template="python_data_analysis",
        context={"task": task_description, "data": data_context}
    )

    # Validate with ruff + mypy
    validator = CodeValidator()
    lint_result = validator.validate(code)

    return {
        "code": code,
        "lint_status": lint_result.status,
        "warnings": lint_result.warnings,
        "safe_to_execute": lint_result.passed
    }
```

---

### Step 3: Register Tools in Agent Tools

**File:** `src/agent_tools.py`

**Add imports:**
```python
from src.tools.verification_tools import (
    validate_calculation_tool,
    verify_financial_data_tool,
    verify_data_quality_tool,
    verify_html_output_tool
)
from src.tools.codegen_tools import (
    generate_python_code_tool,
    generate_sql_query_tool,
    validate_code_tool,
    execute_code_safe_tool
)
```

**Add to AGENT_TOOLS dict:**
```python
AGENT_TOOLS = {
    # ... existing tools ...

    # v0.5.0: Verification tools
    "validate_calculation": validate_calculation_tool,
    "verify_financial_data": verify_financial_data_tool,
    "verify_data_quality": verify_data_quality_tool,
    "verify_html_output": verify_html_output_tool,

    # v0.5.0: Code generation tools
    "generate_python_code": generate_python_code_tool,
    "generate_sql_query": generate_sql_query_tool,
    "validate_code": validate_code_tool,
    "execute_code_safe": execute_code_safe_tool,
}
```

---

### Step 4: Update personal_assistant Configuration

**File:** `ai-bot/bots/personal_assistant.json`

**Add v0.5.0 tools to tools dict:**
```json
{
  "tools": {
    "builtin": ["WebSearch", "WebFetch", "Read", "Grep", "Glob", "Task", "Bash"],
    "campfire": [
      "search_conversations",
      "get_user_context",
      "save_user_preference",
      "manage_personal_tasks",
      "set_reminder",
      "save_personal_note",
      "search_personal_notes"
    ],
    "skills": ["load_skill", "load_skill_file"],
    "verification": [
      "validate_calculation",
      "verify_financial_data",
      "verify_data_quality",
      "verify_html_output"
    ],
    "codegen": [
      "generate_python_code",
      "generate_sql_query",
      "validate_code",
      "execute_code_safe"
    ]
  }
}
```

---

### Step 5: Update Bot System Prompt

**Option A: Update JSON system_prompt**

Add to `ai-bot/bots/personal_assistant.json`:
```
## v0.5.0: Verification & Code Generation

You have access to new capabilities for quality assurance and code generation:

**Verification Tools:**
- validate_calculation(operation, inputs, expected_result) - Verify math is correct
- verify_financial_data(data) - Check financial balance equations
- verify_data_quality(data) - Detect missing/invalid data
- verify_html_output(html) - Validate HTML structure

**When to use verification:**
- Before presenting financial calculations, verify the math
- When generating HTML, validate structure
- When processing user data, check for quality issues

**Code Generation Tools:**
- generate_python_code(task_description, data_context) - Generate Python analysis code
- validate_code(code) - Lint with ruff + mypy
- execute_code_safe(code) - Run in sandbox (30s timeout)

**When to use code generation:**
- User asks for data analysis that requires computation
- Repetitive data processing tasks
- Custom calculations beyond simple math

**Important:**
- Always validate generated code before offering to execute
- Explain what the code does before running
- Use templates (don't write code from scratch)
```

**Option B: Create file-based prompt** (Better for v0.5.0)

Create `prompts/bots/personal_assistant.md` with v0.5.0 section.

---

### Step 6: Create Decorator Files

**File:** `src/tools/verification_decorators.py` (NEW)

Purpose: MCP decorator wrappers for verification module functions

**File:** `src/tools/codegen_decorators.py` (NEW)

Purpose: MCP decorator wrappers for code generation module functions

---

## Testing Plan

### Unit Tests (Already Passing)
- ✅ 179 verification tests
- ✅ 12 codegen tests

### Integration Tests (New)

**Test 1: Verification Tool Accessible**
```python
# tests/agent_behaviors/test_v0.5.0_integration.py
async def test_validation_tool_exists():
    from src.agent_tools import AGENT_TOOLS
    assert "validate_calculation" in AGENT_TOOLS
```

**Test 2: Verification Catches Error**
```python
async def test_verification_catches_impossible_margin():
    tool = AGENT_TOOLS["validate_calculation"]
    result = await tool(
        operation="profit_margin",
        inputs={"revenue": 100, "cost": 120},
        expected_result=0.2
    )
    assert result["valid"] == False
    assert "Cost exceeds revenue" in result["errors"][0]
```

**Test 3: Code Generation Works**
```python
async def test_code_generation_produces_valid_python():
    tool = AGENT_TOOLS["generate_python_code"]
    result = await tool(
        task_description="Calculate average of list",
        data_context={"data": [1, 2, 3, 4, 5]}
    )
    assert result["lint_status"] == "passed"
    assert "def calculate_average" in result["code"]
```

---

## Manual Testing (In Campfire)

**Test 1: Verification**
```
@personal_assistant Calculate profit margin: Revenue $100, Cost $120
```
Expected: Bot uses validate_calculation tool, catches error, warns user

**Test 2: Code Generation**
```
@personal_assistant Generate Python code to calculate average of [10, 20, 30, 40, 50]
```
Expected: Bot uses generate_python_code tool, shows code, offers to execute

**Test 3: Combined**
```
@personal_assistant Analyze this data and verify the results are correct: Revenue $500, Cost $300
```
Expected: Bot calculates margin, verifies with tool, presents validated result

---

## Deployment Steps

### Step 1: Build New Docker Image

```bash
cd /Users/heng/Development/campfire/ai-bot
docker build -t hengwoo/campfire-ai-bot:0.5.0-local .
```

### Step 2: Update docker-compose.dev.yml

```yaml
ai-bot:
  image: hengwoo/campfire-ai-bot:0.5.0-local
  # ... rest of config ...
```

### Step 3: Restart Services

```bash
cd /Users/heng/Development/campfire
docker-compose -f docker-compose.dev.yml down
docker-compose -f docker-compose.dev.yml up -d
```

### Step 4: Verify Integration

```bash
# Check modules loaded
docker exec campfire-ai-bot-dev python -c "from src.tools.verification_tools import *; print('Verification tools loaded')"

# Check tools registered
docker logs campfire-ai-bot-dev | grep "verification\|codegen"
```

---

## File Changes Summary

| File | Type | Purpose |
|------|------|---------|
| `src/tools/verification_decorators.py` | NEW | MCP wrappers for verification |
| `src/tools/codegen_decorators.py` | NEW | MCP wrappers for code generation |
| `src/agent_tools.py` | EDIT | Register v0.5.0 tools |
| `bots/personal_assistant.json` | EDIT | Add v0.5.0 tools to config |
| `tests/agent_behaviors/test_v0.5.0_integration.py` | NEW | Integration tests |

---

## Success Criteria

**Integration Complete When:**
- [ ] Verification tools callable from agent
- [ ] Code generation tools callable from agent
- [ ] Integration tests passing
- [ ] Bot can use tools in Campfire
- [ ] Manual test scenarios pass
- [ ] No regressions in existing functionality

**Production Ready When:**
- [ ] All integration tests pass
- [ ] 5 manual test scenarios pass
- [ ] Performance overhead ≤10%
- [ ] No critical bugs found
- [ ] Documentation updated

---

## Estimated Timeline

- **Step 1-2:** Create tool files (1-2 hours)
- **Step 3:** Register tools (15 min)
- **Step 4-5:** Update bot config (30 min)
- **Step 6:** Write tests (1 hour)
- **Step 7:** Build and deploy (30 min)
- **Step 8:** Manual testing (1-2 hours)

**Total:** 4-6 hours

---

## Next Steps

1. Create `src/tools/verification_decorators.py`
2. Create `src/tools/codegen_decorators.py`
3. Update `src/agent_tools.py`
4. Update `bots/personal_assistant.json`
5. Build Docker image
6. Test in Campfire

---

**Version:** 1.0
**Created:** 2025-10-31
**Status:** Ready to implement
