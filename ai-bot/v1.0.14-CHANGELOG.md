# v1.0.14 - Request Queue System (Race Condition Fix)

**Date:** 2025-10-13
**Status:** Built (pending testing and deployment)
**Critical:** YES - Fixes race conditions in group chat

---

## Problem Statement

In v1.0.13, we identified a critical race condition:

**Issue:** Multiple users in a group chat sending messages simultaneously caused:
- Same cached agent instance accessed by multiple threads concurrently
- Conversation state corruption (agent confused about who it's talking to)
- Mixed responses (Agent A's response bleeding into Agent B's conversation)

**Root Cause:** `agent_cache.get_or_create(room_id, bot_id)` returns the SAME `ClaudeSDKClient` instance for all concurrent requests in the same room, but the client is NOT thread-safe.

**User Confirmation:** User confirmed this was "absolutely critical" to fix before deployment.

---

## Solution: Request Queue System

### Architecture

Implemented a per-agent request queue to serialize concurrent access:

```
Request 1 (User A) ‚Üí Check if busy ‚Üí Acquire lock ‚Üí Process ‚Üí Release lock
Request 2 (User B) ‚Üí Check if busy ‚Üí Wait for lock ‚Üí Process ‚Üí Release lock
Request 3 (User C) ‚Üí Check if busy ‚Üí Wait for lock ‚Üí Process ‚Üí Release lock
```

**Key:** Each `(room_id, bot_id)` pair gets its own lock. Different rooms or different bots can process in parallel, but multiple requests to the SAME agent in the SAME room are serialized.

---

## Changes Made

### 1. New File: `src/request_queue.py`

**RequestQueue Class:**
- `get_lock(room_id, bot_id)` - Get or create lock for agent
- `is_busy(room_id, bot_id)` - Non-blocking check if agent is processing
- `acquire(room_id, bot_id, blocking, timeout)` - Acquire lock (blocks if busy)
- `release(room_id, bot_id)` - Release lock after processing
- `get_stats()` - Get queue statistics

**Features:**
- Singleton pattern via `get_request_queue()`
- Per-agent locks (not global lock)
- Statistics tracking (total requests, queued requests, active agents)
- 5-minute timeout to prevent deadlock

---

### 2. Modified: `src/app.py`

#### Added Import
```python
from request_queue import get_request_queue
```

#### New Endpoint: `/queue/stats`
Returns queue statistics:
```json
{
  "total_requests": 42,
  "queued_requests": 7,
  "active_agents": 2,
  "active_keys": [[1, "financial_analyst"], [5, "technical_assistant"]],
  "timestamp": "2025-10-13T14:30:00Z"
}
```

#### Modified `process_in_background()` Function

**Before processing:**
```python
# Check if busy
if request_queue.is_busy(room_id, bot_config.bot_id):
    # Post waiting message
    post_to_campfire(room_id, "‚è≥ I'm currently helping someone else...")

# Acquire lock (blocks until available)
acquired = request_queue.acquire(
    room_id=room_id,
    bot_id=bot_config.bot_id,
    blocking=True,
    timeout=300  # 5 minutes
)

if not acquired:
    # Timeout - couldn't get lock
    post_to_campfire(room_id, "‚ö†Ô∏è Sorry, I'm overloaded...")
    return
```

**After processing (in finally block):**
```python
finally:
    # Release lock - CRITICAL
    request_queue.release(room_id, bot_config.bot_id)

    # Clean up async tasks...
```

---

### 3. Modified: `Dockerfile`

Updated version to 1.0.14:
```dockerfile
# v1.0.14: Request queue system - prevents race conditions in group chat, serializes concurrent agent access
LABEL version="1.0.14"
```

---

## User Experience

### Scenario 1: Single User (No Change)
- User sends message
- Agent processes immediately
- Response appears in Campfire

### Scenario 2: Concurrent Users (NEW)

**User A sends message:**
- ‚úÖ "ü§î Processing your request..."
- Agent processes
- Response appears

**User B sends message (while A is processing):**
- ‚è≥ "I'm currently helping someone else in this room. Your request will be processed shortly..."
- Waits for User A to finish
- ‚úÖ "ü§î Processing your request..."
- Agent processes
- Response appears

**User C sends message (while B is waiting):**
- ‚è≥ "I'm currently helping someone else..."
- Waits in queue
- Eventually processes

---

## Benefits

‚úÖ **Thread Safety:** No more race conditions - one request at a time per agent
‚úÖ **Conversation Integrity:** Agent maintains correct context for each user
‚úÖ **Transparent Queueing:** Users see waiting message instead of confusion
‚úÖ **Deadlock Prevention:** 5-minute timeout prevents infinite waits
‚úÖ **Parallel Processing:** Different rooms/bots still process in parallel
‚úÖ **Monitoring:** `/queue/stats` endpoint for debugging

---

## Technical Details

### Lock Granularity
- **NOT** a global lock (that would serialize ALL requests)
- **Per-agent locks:** `(room_id, bot_id)` is the key
- **Parallel processing:** Room 1 + Room 2 can both process simultaneously
- **Serial processing:** Room 1 + User A/B/C process one at a time

### Timeout Behavior
- **Default timeout:** 5 minutes (300 seconds)
- **After timeout:** Post error message and abort
- **Prevents:** Infinite waiting if agent crashes mid-processing

### Statistics Tracking
```python
{
    'total_requests': 100,      # All requests ever
    'queued_requests': 15,      # Requests that had to wait
    'active_agents': 2,         # Currently processing
    'active_keys': [(1, 'bot'), (5, 'bot')]  # Which agents are busy
}
```

---

## Testing Plan

### Unit Tests
```bash
# Test queue basics
- Acquire lock ‚Üí should succeed
- Try acquire again ‚Üí should block
- Release lock ‚Üí should allow next request
```

### Integration Tests
```bash
# Test concurrent requests
1. Send 3 requests to same room simultaneously
2. Verify only 1 processes at a time
3. Verify all 3 eventually complete
4. Verify responses don't mix
```

### Load Tests
```bash
# Test under load
1. Send 20 requests to same room
2. Verify queue handles gracefully
3. Check /queue/stats shows correct counts
4. Verify no deadlock after 20 requests
```

---

## Deployment Notes

### Building
```bash
cd /Users/heng/Development/campfire/ai-bot
docker buildx build --platform linux/amd64 \
  -t hengwoo/campfire-ai-bot:1.0.14 \
  -t hengwoo/campfire-ai-bot:latest .
```

### Pushing
```bash
docker push hengwoo/campfire-ai-bot:1.0.14
docker push hengwoo/campfire-ai-bot:latest
```

### Deploying on Server
```bash
# On DigitalOcean server
docker-compose down
docker pull hengwoo/campfire-ai-bot:1.0.14
# Edit docker-compose.yml: change image to 1.0.14
docker-compose up -d
docker logs -f campfire-ai-bot
```

---

## Monitoring

### Check Queue Status
```bash
curl http://localhost:5000/queue/stats
```

### Watch Logs for Queue Activity
```bash
docker logs -f campfire-ai-bot | grep Queue
```

Expected log output:
```
[Queue] Agent for room 1 is busy - posted waiting message
[Queue] Waiting for lock for room 1, bot financial_analyst
[Queue] Acquired lock for room 1, bot financial_analyst
[Queue] Releasing lock for room 1, bot financial_analyst
```

---

## Known Limitations

1. **Queue is in-memory:** Restarting the container clears queue state
2. **No persistent queue:** Requests in queue when container restarts are lost
3. **No cross-server coordination:** If we scale to multiple servers, each has its own queue

**Future Improvements (if needed):**
- Redis-based distributed queue for multi-server deployment
- Persistent queue storage
- Priority queue (VIP users first)

---

## Version History

| Version | Feature | Status |
|---------|---------|--------|
| v1.0.13 | Streaming responses + race condition identified | Built, NOT deployed |
| v1.0.14 | Request queue system to fix race conditions | Built, pending testing |

---

## Next Steps

1. ‚úÖ Build Docker image (complete)
2. ‚è≥ Push to Docker Hub (pending)
3. ‚è≥ Deploy to production server (pending)
4. ‚è≥ Test concurrent user scenario (pending)
5. ‚è≥ Monitor queue statistics (pending)
6. ‚è≥ Verify no race conditions in production (pending)

---

**Author:** Claude AI Assistant + Wu Heng
**Date:** 2025-10-13
**Criticality:** HIGH - Blocks v1.0.13 deployment due to race condition risk
